{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "sys.path.append(os.getcwd()+'/model/')\n",
    "sys.path.append(os.getcwd()+'/losses/')\n",
    "sys.path.append(os.getcwd()+'/Dataloader/')\n",
    "\n",
    "\n",
    "from Encoder_model import *\n",
    "from Classifier_Model import *\n",
    "from Decoder_model import *\n",
    "from Comparator_model import *\n",
    "from Dense_Spatial_Transformation import * \n",
    "from MorphSSL_dataloader import *\n",
    "from TTC_dataloader import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75d1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbbaa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_inference(val_loader):\n",
    "    classifier_model.eval()\n",
    "    feature_model.eval()\n",
    "    \n",
    "    gt_lst=[]\n",
    "    pred_lst=[]\n",
    "    a_lst=[]\n",
    "    b_lst=[]\n",
    "    \n",
    "    \n",
    "    cnt=0\n",
    "    for i, sample in enumerate(val_loader):     \n",
    "        \n",
    "        img=sample['I']\n",
    "        gt=sample['gt'] # It has 2 points\n",
    "        t=sample['t']\n",
    "        t_cnv=sample['t_cnv']\n",
    "        t_bfr=sample['t_bfr']\n",
    "        \n",
    "        img=img.to(device)\n",
    "        gt=gt.to(device)\n",
    "        t=t.to(device)\n",
    "        t_cnv=t_cnv.to(device)\n",
    "        t_bfr=t_bfr.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ftr=feature_model(img)\n",
    "            a,b,_=classifier_model(ftr)\n",
    "            pred=torch.sigmoid((t-b)/(a + 0.05))\n",
    "        \n",
    "        \n",
    "        pred=pred.detach().cpu().numpy()\n",
    "        gt=gt.detach().cpu().numpy()\n",
    "        a=a.detach().cpu().numpy()\n",
    "        b=b.detach().cpu().numpy()\n",
    "        \n",
    "        pred_lst.append(pred)\n",
    "        gt_lst.append(gt)\n",
    "        a_lst.append(a)\n",
    "        b_lst.append(b)\n",
    "        del pred, gt,a,b\n",
    "        \n",
    "    \n",
    "    pred_lst=np.concatenate(pred_lst, axis=0) # B,4\n",
    "    gt_lst=np.concatenate(gt_lst, axis=0) # B,4\n",
    "    a_lst=np.concatenate(a_lst, axis=0) # B,1\n",
    "    b_lst=np.concatenate(b_lst, axis=0) # B,1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### ROC_AUC for 0 month, 6 month, 12 month, 18 month\n",
    "    tmp_gt=np.squeeze(gt_lst[:,0])\n",
    "    tmp_pred=np.squeeze(pred_lst[:,0])\n",
    "    idx=np.where(tmp_gt!=-1)\n",
    "    tmp_gt=tmp_gt[idx]\n",
    "    tmp_pred=tmp_pred[idx]\n",
    "    roc_auc0=roc_auc_score(tmp_gt, tmp_pred)\n",
    "    \n",
    "    tmp_gt=np.squeeze(gt_lst[:,1])\n",
    "    tmp_pred=np.squeeze(pred_lst[:,1])\n",
    "    idx=np.where(tmp_gt!=-1)\n",
    "    tmp_gt=tmp_gt[idx]\n",
    "    tmp_pred=tmp_pred[idx]\n",
    "    roc_auc1=roc_auc_score(tmp_gt, tmp_pred)\n",
    "    \n",
    "    tmp_gt=np.squeeze(gt_lst[:,2])\n",
    "    tmp_pred=np.squeeze(pred_lst[:,2])\n",
    "    idx=np.where(tmp_gt!=-1)\n",
    "    tmp_gt=tmp_gt[idx]\n",
    "    tmp_pred=tmp_pred[idx]\n",
    "    roc_auc2=roc_auc_score(tmp_gt, tmp_pred)\n",
    "    \n",
    "    tmp_gt=np.squeeze(gt_lst[:,3])\n",
    "    tmp_pred=np.squeeze(pred_lst[:,3])\n",
    "    idx=np.where(tmp_gt!=-1)\n",
    "    tmp_gt=tmp_gt[idx]\n",
    "    tmp_pred=tmp_pred[idx]\n",
    "    roc_auc3=roc_auc_score(tmp_gt, tmp_pred)\n",
    "    \n",
    "    del tmp_gt, tmp_pred\n",
    "    \n",
    "    \n",
    "    \n",
    "    metric=(roc_auc0+roc_auc1+roc_auc2+roc_auc3)/4\n",
    "    \n",
    "    print(\"ROC AUC, 0 mnth: {:.4f}, 6 mnth: {:.4f}, 12 mnth: {:.4f}, 18 mnth: {:.4f}\"\n",
    "          .format(roc_auc0, roc_auc1, roc_auc2, roc_auc3))\n",
    "    \n",
    "    classifier_model.train()\n",
    "    if trn_type=='finetune':\n",
    "        feature_model.train()\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3e56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9513973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(a,b,t,gt,t_cnv,t_bfr, roi_msk, cam):\n",
    "    \n",
    "    # GT : 0 for not converted, 1 for converted. Inverse of Survival\n",
    "    # Get the prediction\n",
    "    pred=torch.sigmoid((t-b)/(a + 0.05)) \n",
    "    loss_pred=F.binary_cross_entropy(pred, gt,reduction='none') \n",
    "    loss_pred=torch.mean(loss_pred, dim=1)\n",
    "    loss_pred=torch.squeeze(torch.mean(loss_pred))\n",
    "    \n",
    "    # Cam minimize region outside the ROI\n",
    "    loss_cam=cam*(1-roi_msk)# B,1,H,W,D\n",
    "    loss_cam=loss_cam.view(-1) # (BHWD)\n",
    "    loss_cam=torch.squeeze(torch.mean((loss_cam**2),dim=0))\n",
    "    \n",
    "    ########\n",
    "    loss_uncrtn=torch.squeeze(torch.mean((a.view(-1)**2),dim=0))\n",
    "    \n",
    "    loss=1.0*loss_pred+0.1*loss_uncrtn+0.1*loss_cam\n",
    "    return loss, loss_pred, loss_uncrtn, loss_cam\n",
    "    \n",
    "    \n",
    "\n",
    "def train_one_batch(img,t,gt,t_cnv,t_bfr,roi_msk, optimizer1, scheduler1,optimizer2, scheduler2):\n",
    "    if trn_type=='finetune':\n",
    "        ftr=feature_model(img)\n",
    "    elif trn_type=='freeze':\n",
    "        with torch.no_grad():\n",
    "            ftr=feature_model(img)\n",
    "    \n",
    "    a,b,cam=classifier_model(ftr) # unnormalized score without sigmoid\n",
    "    loss, loss_pred, loss_uncrtn, loss_cam=compute_loss(a,b,t,gt,t_cnv,t_bfr, roi_msk, cam)\n",
    "    \n",
    "    ###################### Backpropagation ###########################\n",
    "    # remove previous gradients\n",
    "    optimizer1.zero_grad()\n",
    "    if trn_type=='finetune':\n",
    "        optimizer2.zero_grad()\n",
    "    # compute the gradients\n",
    "    loss.backward() \n",
    "    # Update the weights\n",
    "    optimizer1.step()\n",
    "    # Update Learning rate scheduler\n",
    "    scheduler1.step()\n",
    "    \n",
    "    if trn_type=='finetune':\n",
    "        optimizer2.step()\n",
    "        scheduler2.step()\n",
    "    \n",
    "    loss=loss.detach().cpu().numpy()\n",
    "    loss_pred=loss_pred.detach().cpu().numpy()\n",
    "    loss_uncrtn=loss_uncrtn.detach().cpu().numpy()\n",
    "    loss_cam=loss_cam.detach().cpu().numpy()\n",
    "    \n",
    "    return loss, loss_pred, loss_uncrtn, loss_cam,optimizer1,scheduler1,optimizer2, scheduler2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94526a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_complete():\n",
    "    mx_lr=10**(-5.0) \n",
    "    mn_lr=10**(-6.0)     \n",
    "    total_batches=len(train_loader) \n",
    "    ################################################################################################# \n",
    "    cycle_length=240\n",
    "    max_epochs=1000 # First No. is no. of complete triangles to train\n",
    "    tot_batch_updates_val= 2*cycle_length # check at the end of \"scheduler\" triangles where lr is min.\n",
    "    \n",
    "    patience=100 # Stop training if val metric doesnot improve for \"patience\" successive validation checks\n",
    "    \n",
    "    #################################################################################################\n",
    "    \n",
    "    optimizer1=torch.optim.AdamW(classifier_model.parameters(), lr=mn_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=wt_decay)\n",
    "    \n",
    "    scheduler1=torch.optim.lr_scheduler.CyclicLR(optimizer1, base_lr=mn_lr, max_lr=mx_lr, cycle_momentum=False,\n",
    "                                            step_size_up=cycle_length, step_size_down=None, mode='triangular')\n",
    "    \n",
    "    if trn_type=='finetune':\n",
    "        optimizer2=torch.optim.AdamW(feature_model.parameters(), lr=mn_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=wt_decay)\n",
    "    \n",
    "        scheduler2=torch.optim.lr_scheduler.CyclicLR(optimizer2, base_lr=mn_lr, max_lr=mx_lr, cycle_momentum=False,\n",
    "                                            step_size_up=cycle_length, step_size_down=None, mode='triangular')\n",
    "    elif trn_type=='freeze':\n",
    "        optimizer2=None\n",
    "        scheduler2=None\n",
    "    \n",
    "    ##### Initial metric ########\n",
    "    metric=complete_inference(val_loader)\n",
    "    \n",
    "    #### Initialize ######\n",
    "    ptnc_cnt=0 # for Early stopping\n",
    "    flag=0 # 0=> continue, 1=> Early Stopping has occured to break the outer loop for epoch\n",
    "    max_metric=-999 # best val accuracy encountered so far\n",
    "    \n",
    "    cnt=0\n",
    "    run_loss=0\n",
    "    run_loss_pred=0\n",
    "    run_loss_uncrtn=0\n",
    "    run_loss_cam=0\n",
    "    \n",
    "    t2 = time.time()\n",
    "    \n",
    "    for epoch in range(0, max_epochs):\n",
    "        for i, sample in enumerate(train_loader):\n",
    "            cnt=cnt+1\n",
    "            \n",
    "            img=sample['I']\n",
    "            t=sample['t']\n",
    "            gt=sample['gt']\n",
    "            t_cnv=sample['t_cnv']\n",
    "            t_bfr=sample['t_bfr']\n",
    "            roi_msk=sample['roi_msk']\n",
    "            del sample\n",
    "            \n",
    "            img=img.to(device)\n",
    "            t=t.to(device)\n",
    "            gt=gt.to(device)\n",
    "            roi_msk=roi_msk.to(device)\n",
    "            \n",
    "            loss, loss_pred, loss_uncrtn, loss_cam,optimizer1,scheduler1,optimizer2,scheduler2=train_one_batch(img,t,gt,t_cnv,t_bfr, roi_msk, optimizer1, scheduler1,optimizer2, scheduler2)\n",
    "            \n",
    "            run_loss=run_loss+loss\n",
    "            run_loss_pred=run_loss_pred+loss_pred\n",
    "            run_loss_uncrtn=run_loss_uncrtn+loss_uncrtn\n",
    "            run_loss_cam=run_loss_cam+loss_cam\n",
    "            \n",
    "            del img, gt, t, loss, loss_pred, loss_uncrtn, loss_cam\n",
    "            \n",
    "            if (i+1) % 10== 0: # displays after every 10 batch updates\n",
    "                print (\"Epoch [{}/{}], Batch [{}/{}], cnt {}, Train Loss: {:.4f}, Loss Pred: {:.4f}, Loss Uncrtn: {:.4f}, Loss CAM: {:.4f}\"\n",
    "                       .format(epoch+1, max_epochs, i+1, total_batches, cnt, (run_loss/cnt),(run_loss_pred/cnt),(run_loss_uncrtn/cnt),(run_loss_cam/cnt)), end =\"\\r\")\n",
    "            \n",
    "            ############# Monitor Validation Acc and Early Stopping ############\n",
    "            if cnt>=tot_batch_updates_val:\n",
    "                print('\\n Training time for 1 cycle is: '+str(time.time() - t2) +' seconds')\n",
    "                # Monitor val auc\n",
    "                metric=complete_inference(val_loader)\n",
    "                \n",
    "                ########## Reinitialize for next cycle the running loss, optimizer and scheduler\n",
    "                cnt=0\n",
    "                run_loss=0\n",
    "                run_loss_pred=0\n",
    "                run_loss_uncrtn=0\n",
    "                run_loss_cam=0\n",
    "                t2 = time.time()\n",
    "            \n",
    "                # Early Stopping\n",
    "                if metric>max_metric:\n",
    "                    torch.save({'classifier_model_state_dict_model': classifier_model.state_dict(),\n",
    "                                'feature_model_state_dict_model': feature_model.state_dict()},\n",
    "                               wt_nm)\n",
    "                    max_metric=metric\n",
    "                    ptnc_cnt=0\n",
    "                else:\n",
    "                    ptnc_cnt=ptnc_cnt+1\n",
    "                    print('\\n Validation metric has not improved in last '+str(ptnc_cnt)+' batch updates')\n",
    "                    if ptnc_cnt>=patience:\n",
    "                        print(\"\\n Early Stopping ! \\n\")\n",
    "                        flag=1 # this will be used to break out of the outer loop for epochs.\n",
    "                        break # this breaks out of inner loop of Dataloader\n",
    "        if flag==1: # Early Stopping has ocurred\n",
    "            break # break out of the outer loop for epochs.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f834b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e267a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04517b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code was modified to perform both training with the encoder model frozen and end-to-end finetuning.\n",
    "Originally, the frozen experiments were performed separately after pre-extracting 10 different data-augmented versions\n",
    "of each sample with the encoder and using these features during training the classifier. This precomputation is faster \n",
    "and requires less computation as the forward pass through the encoder was precomputed and was not needed to be performed\n",
    "while training.\n",
    "\"\"\"\n",
    "\n",
    "eps=10**(-9)\n",
    "fld=3 # the fold for which to train\n",
    "wt_decay=0.01\n",
    "trn_type='freeze' # or 'finetune'. 'freeze' should always be run before running 'finetune'\n",
    "\n",
    "#################### Dataloaders #####################################\n",
    "fld_pth='/msc/home/achakr83/PINNACLE/SSL_training/June30/cross_validation_splits_new/fold'+str(fld)+'.npz' \n",
    "# fld_pth is the pth to a npz file containing the list of scans in the current fold and GT. \n",
    "# See /Dataloader/TTC_dataloader/  for details.\n",
    "\n",
    "# directory containing the preprocessed OCT scans in npz files.\n",
    "img_pth='/msc/home/achakr83/PINNACLE/preprocessed_Downstream_images2/' \n",
    "\n",
    "train_data=train_dataset(fold=fld_pth, img_pth=img_pth)\n",
    "train_loader=DataLoader(dataset=train_data, batch_size=3, shuffle=True, num_workers=2, \n",
    "                             pin_memory=False, drop_last=False)\n",
    "    \n",
    "val_data=val_dataset(fold=fld, img_pth=img_pth)\n",
    "val_loader=DataLoader(dataset=val_data, batch_size=3, shuffle=False, num_workers=2, \n",
    "                             pin_memory=False, drop_last=False, worker_init_fn=worker_init_fn)\n",
    "\n",
    "\n",
    "################## Prepare the model  ############################### \n",
    "feature_model=Encoder_Architecture(base_chnls=16, out_ftr_dim=(64*2))\n",
    "feature_model.to(device)\n",
    "\n",
    "classifier_model=Classification_Network()\n",
    "classifier_model.to(device)\n",
    "\n",
    "################## Load the feature_model ##############################\n",
    "checkpoint = torch.load('best_weight-0.007980789116118103.pt')\n",
    "feature_model.load_state_dict(checkpoint['model_state_dict_encoder_model'])\n",
    "del checkpoint\n",
    "\n",
    "if trn_type=='finetune':\n",
    "    # Additionally load initial weights for the classifier, obtained by training with the frozen model wts first.\n",
    "    checkpoint = torch.load(str(fld)+'_best_weight_frozen.pt')\n",
    "    classifier_model.load_state_dict(checkpoint['classifier_model_state_dict_model'])\n",
    "    del checkpoint    \n",
    "    wt_nm=str(fld)+'_best_weight_finetune.pt'\n",
    "elif trn_type=='freeze':\n",
    "    wt_nm=str(fld)+'_best_weight_frozen.pt'\n",
    "    # freeze the weights of the feature_model(encoder)\n",
    "    for param in feature_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    feature_model.eval() # its features are frozen\n",
    "    \n",
    "\n",
    "\n",
    "train_complete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
